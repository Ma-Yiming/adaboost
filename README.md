# adaboost
## 目的
组合相似的分类器来提高分类性能、掌握Adaboost算法原理内容、基于单层决策树构建弱分类器，并在一个难数据集上应用AdaBoost、使用其它弱分类器完成AdaBoost算法。
## 文件结构
本实验为基于AdaBoost的实验，前两个实验需要对决策树桩进行集成，最后一个实验自选一种基分类器，在这里我选择的是CART决策树。由于前两个实验都是基于分类树桩，所以我构建了一个基于分类树桩的AdaBoost类，两个实验共用了这个类。而最后一个实验因为只有一个，所以我没有构建类，直接进行了编程。其中，分类树桩AdaBoost集成类对应文件为adaclass.py、简单数据集测试文件为simpledata.py、horseColic分类的文件为horseColic.py、基于CART决策树的AdaBoost集成文件为CART.py  
## 流程
### 基于单层决策树桩构建弱分类器
单层决策树桩很好理解，就是一个分段函数，需要有一个分类特征，分类值和一个决策操作，当当前特征满足操作下的分类值时，我们将它记为某个数，当不满足时，我们记为另一个数。
### 完整的AdaBoost实现和测试算法
这里完整的实现就是开始时的StumpAdaBoost类的实现，其中包括基分类器的定义、选择最佳基分类器、基分类器对应权重和样本权重的更新、预测这几个函数。
### 基于决策树桩的集成对horseColic的测试
首先实对horseColic数据进行导入，之后，我们便可以利用我们已经定义好的StumpAdaBoost类进行test。
### 基于CART的AdaBoost
构建CART弱分类器的集成主要是计算最大的基尼系数。因为CART中基尼系数表示的意义是从数据集D中随机抽取两个样本，其类别标记不一致的概率。计算基尼系数的过程中会牵扯到不同类别数据的概率，而训练数据的权重表示每条数据的重要程度，进而可以看作是该训练数据出现的概率（即权重大的数据出现的概率大），所以就可以在计算基尼系数对数据进行分类时，将不同类别里的数据乘上它的权重再计算概率，这样得到的基尼系数就可以将权重结合起来了。不管是AdaBoost算法还是其等价形式前向分步算法，他们迭代的思想始终都是极小化损失函数，也就是尽可能的使每一次迭代得到的弱分类器的分类误差最小。而采用多数表决，虽然有可能使弱分类器不像一个“分类器”，但可以极小化损失函数，从而使最终线性组合的基本分类器有一个较好的分类效果。
